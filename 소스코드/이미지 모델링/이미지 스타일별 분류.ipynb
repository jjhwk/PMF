{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ccffcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73139674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5994 images belonging to 8 classes.\n",
      "Found 749 images belonging to 8 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 795ms/step - accuracy: 0.1665 - loss: 2.5508 - val_accuracy: 0.2065 - val_loss: 1.9814\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:46\u001b[0m 767ms/step - accuracy: 0.1250 - loss: 3.0786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.1250 - loss: 3.0786 - val_accuracy: 0.2308 - val_loss: 1.9430\n",
      "Epoch 3/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 827ms/step - accuracy: 0.1775 - loss: 2.3505 - val_accuracy: 0.2011 - val_loss: 2.0565\n",
      "Epoch 4/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1250 - loss: 2.1246 - val_accuracy: 0.3077 - val_loss: 1.8860\n",
      "Epoch 5/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 815ms/step - accuracy: 0.2099 - loss: 2.2478 - val_accuracy: 0.2283 - val_loss: 2.1959\n",
      "Epoch 6/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.1875 - loss: 2.5865 - val_accuracy: 0.5385 - val_loss: 1.9432\n",
      "Epoch 7/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 822ms/step - accuracy: 0.2143 - loss: 2.1990 - val_accuracy: 0.2255 - val_loss: 2.1240\n",
      "Epoch 8/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.1250 - loss: 2.2207 - val_accuracy: 0.1538 - val_loss: 2.0039\n",
      "Epoch 9/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 805ms/step - accuracy: 0.2240 - loss: 2.1298 - val_accuracy: 0.2337 - val_loss: 1.9876\n",
      "Epoch 10/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3750 - loss: 1.7202 - val_accuracy: 0.0769 - val_loss: 1.9848\n",
      "Epoch 1/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.2465 - loss: 2.1452\n",
      "Epoch 1: val_accuracy improved from -inf to 0.13315, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1375s\u001b[0m 4s/step - accuracy: 0.2467 - loss: 2.1446 - val_accuracy: 0.1332 - val_loss: 2.8292\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24:05\u001b[0m 4s/step - accuracy: 0.4375 - loss: 1.1491\n",
      "Epoch 2: val_accuracy improved from 0.13315 to 0.23077, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.4375 - loss: 1.1491 - val_accuracy: 0.2308 - val_loss: 2.3676\n",
      "Epoch 3/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4670 - loss: 1.4414\n",
      "Epoch 3: val_accuracy improved from 0.23077 to 0.23098, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1301s\u001b[0m 3s/step - accuracy: 0.4671 - loss: 1.4413 - val_accuracy: 0.2310 - val_loss: 2.7451\n",
      "Epoch 4/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:04\u001b[0m 3s/step - accuracy: 0.7500 - loss: 1.1166\n",
      "Epoch 4: val_accuracy improved from 0.23098 to 0.46154, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7500 - loss: 1.1166 - val_accuracy: 0.4615 - val_loss: 2.5233\n",
      "Epoch 5/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.5645 - loss: 1.1786\n",
      "Epoch 5: val_accuracy improved from 0.46154 to 0.49049, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 4s/step - accuracy: 0.5645 - loss: 1.1786 - val_accuracy: 0.4905 - val_loss: 1.4535\n",
      "Epoch 6/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22:34\u001b[0m 4s/step - accuracy: 0.7500 - loss: 0.7749\n",
      "Epoch 6: val_accuracy improved from 0.49049 to 0.53846, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 47ms/step - accuracy: 0.7500 - loss: 0.7749 - val_accuracy: 0.5385 - val_loss: 1.1462\n",
      "Epoch 7/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.6281 - loss: 0.9845\n",
      "Epoch 7: val_accuracy did not improve from 0.53846\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1409s\u001b[0m 4s/step - accuracy: 0.6281 - loss: 0.9845 - val_accuracy: 0.4538 - val_loss: 2.1278\n",
      "Epoch 8/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20:41\u001b[0m 3s/step - accuracy: 0.5625 - loss: 1.3460\n",
      "Epoch 8: val_accuracy did not improve from 0.53846\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step - accuracy: 0.5625 - loss: 1.3460 - val_accuracy: 0.2308 - val_loss: 2.2771\n",
      "Epoch 9/10\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.6724 - loss: 0.9051\n",
      "Epoch 9: val_accuracy improved from 0.53846 to 0.61005, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1393s\u001b[0m 4s/step - accuracy: 0.6724 - loss: 0.9051 - val_accuracy: 0.6101 - val_loss: 1.0976\n",
      "Epoch 10/10\n",
      "\u001b[1m  1/374\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22:13\u001b[0m 4s/step - accuracy: 0.7500 - loss: 0.6118\n",
      "Epoch 10: val_accuracy improved from 0.61005 to 0.61538, saving model to ./model_data/best_model_v4.keras\n",
      "\u001b[1m374/374\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7500 - loss: 0.6118 - val_accuracy: 0.6154 - val_loss: 1.1448\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로 설정\n",
    "# 데이터 경로 설정\n",
    "train_data_dir = 'img_train_test(누끼)/train'\n",
    "validation_data_dir = 'img_train_test(누끼)/val'\n",
    "\n",
    "# 데이터 증강 및 전처리\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2, # 무작위 확대 축소\n",
    "    horizontal_flip=False # 좌우 반전 끔\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 사전 학습된 ResNet-50 모델 로드 (가중치는 ImageNet으로 사전 학습됨)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# 새로운 층 추가\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # 과적합 방지\n",
    "x = Dense(1024)(x)\n",
    "x = BatchNormalization()(x) # 안정성 및 속도 향상\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x) # 과적합 방지\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# 출력 층\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "# 전체 모델 정의\n",
    "model = Model(inputs = base_model.input, outputs = predictions)\n",
    "\n",
    "# 기본 모델의 가중치를 고정\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# 기본 모델의 가중치를 고정 해제\n",
    "# 저희가 가지고 있는 데이터에 더 fit한 모델이 될 수있도록\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# 모델 재컴파일\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델을 저장할 파일명 설정\n",
    "checkpoint_filepath = './model_data/best_model_v4.keras'\n",
    "\n",
    "# ModelCheckpoint 콜백 설정\n",
    "checkpoint = ModelCheckpoint(\n",
    "    checkpoint_filepath,\n",
    "    monitor='val_accuracy', \n",
    "    save_best_only=True, \n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 모델 훈련 (미세 조정)\n",
    "history = model.fit(\n",
    "    train_generator, # 훈련용 데이터셋을 생성\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size, \n",
    "    # 모든 샘플을 한 번씩 사용\n",
    "    \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10,\n",
    "    callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bbc2ac",
   "metadata": {},
   "source": [
    "# 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459a0cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 750 images belonging to 8 classes.\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 716ms/step - accuracy: 0.6121 - loss: 1.0354\n",
      "Test Accuracy: 0.6306666731834412\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델 로드\n",
    "saved_model = load_model('./model_data/best_model_v4.keras')\n",
    "\n",
    "# 테스트 데이터셋 경로 설정\n",
    "test_data_dir = 'img_train_test(누끼)/test'\n",
    "\n",
    "# 테스트 데이터셋에 대한 ImageDataGenerator 생성\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 모델 평가\n",
    "evaluation = saved_model.evaluate(test_generator)\n",
    "\n",
    "print(\"Test Accuracy:\", evaluation[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3963f3",
   "metadata": {},
   "source": [
    "# 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8b590d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000015E53C95440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted class: american\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "# 예측하려는 이미지 경로\n",
    "img1_path = \"./image_nuggi(누끼)/men/american/americancasual_25861.jpg\"\n",
    "# image_path = './누끼.jpg'\n",
    "\n",
    "# 모델 물러오기\n",
    "test_model = load_model(\"./model_data/best_model_v4.keras\")\n",
    "\n",
    "# 이미지 불러오기 및 전처리\n",
    "img = image.load_img(img1_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = img_array / 255.0  # 이미지를 0과 1 사이로 정규화 (훈련할 때와 동일한 전처리)\n",
    "\n",
    "# 예측 생성\n",
    "predictions = test_model.predict(img_array)\n",
    "\n",
    "# 결과 해석\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# class_labels = validation_generator.class_indices\n",
    "class_labels = {'american': 0,    # 이부분만 바꿔줌\n",
    " 'casual': 1,\n",
    " 'chic': 2,\n",
    " 'dandy': 3,\n",
    " 'formal': 4,\n",
    " 'gorpcore': 5,\n",
    " 'sports': 6,\n",
    " 'street': 7}\n",
    "predicted_class_label = list(class_labels.keys())[list(class_labels.values()).index(predicted_class_index)]\n",
    "\n",
    "print(\"Predicted class:\", predicted_class_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
